# Classification Loss Functions

## Topics covered in today's module
* Kullback Leibler Divergence Loss
* Binary Cross-Entropy
* Categorical Cross-Entropy
* Sparse Categorical Cross-Entropy

## Main takeaways from doing today's assignment
* Understanding the differences between Binary Cross-Entropy, Categorical Cross-Entropy, and Sparse Categorical Cross-Entropy.
* Recognizing the use cases of each loss function and their advantages and disadvantages.
* Understanding the mathematical relationships between the loss functions.
* Understanding the importance of choosing the appropriate loss function for the task at hand.


## Challenging, interesting, or exciting aspects of today's assignment
* The challenging aspect was the mathematical concepts behind each loss function. 
* The interesting and exciting aspect was the practical application of these topics in deep learning models.

## Additional resources used 
ðŸ“Œ ["Loss Functions in Deep Learning" by Dan Van Boxel on YouTube](https://www.youtube.com/watch?v=IVVVjBSk9N0)<br>
This video provides an of different loss functions in deep learning, including Binary Cross-Entropy, Categorical Cross-Entropy, and Sparse Categorical Cross-Entropy, with visualizations and code snippets.<br>
ðŸ“Œ ["Demystifying Cross-Entropy Loss" by Avik Jain](https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8)<br>
ðŸ“Œ [  A Brief Overview of Loss Functions in Pytorch](https://medium.com/@mudit13051998/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7)<br>
ðŸ“Œ ["A Gentle Introduction to Cross-Entropy for Machine Learning" by Jason Brownlee](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)<br>
