{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MySureStart/spring_2023_vail-Mennatullah-Elsahy/blob/main/Day_05/Introduction_to_Regression_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 5 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de32fc1b-1ad8-475d-dee7-3ef177930e8b"
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd87ee9-67ac-4aad-f03c-f9cb9ecffa20"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 8s 56ms/step - loss: 592.5969 - mse: 592.5969 - val_loss: 491.4631 - val_mse: 491.4631\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 569.4691 - mse: 569.4691 - val_loss: 470.6776 - val_mse: 470.6776\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 546.0515 - mse: 546.0515 - val_loss: 446.5483 - val_mse: 446.5483\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 514.8578 - mse: 514.8578 - val_loss: 410.5729 - val_mse: 410.5729\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 465.5668 - mse: 465.5668 - val_loss: 354.6135 - val_mse: 354.6135\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 391.9346 - mse: 391.9346 - val_loss: 274.7484 - val_mse: 274.7484\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 296.8525 - mse: 296.8525 - val_loss: 179.8267 - val_mse: 179.8267\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 190.0219 - mse: 190.0219 - val_loss: 101.7799 - val_mse: 101.7799\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 120.3333 - mse: 120.3333 - val_loss: 66.1446 - val_mse: 66.1446\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 90.7781 - mse: 90.7781 - val_loss: 57.8828 - val_mse: 57.8828\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 75.1053 - mse: 75.1053 - val_loss: 48.2508 - val_mse: 48.2508\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 63.5837 - mse: 63.5837 - val_loss: 40.0931 - val_mse: 40.0931\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 53.4747 - mse: 53.4747 - val_loss: 34.6552 - val_mse: 34.6552\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 46.5774 - mse: 46.5774 - val_loss: 30.5402 - val_mse: 30.5402\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 40.4713 - mse: 40.4713 - val_loss: 29.0389 - val_mse: 29.0389\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 36.1170 - mse: 36.1170 - val_loss: 26.2021 - val_mse: 26.2021\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 32.8131 - mse: 32.8131 - val_loss: 24.4931 - val_mse: 24.4931\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 30.5955 - mse: 30.5955 - val_loss: 23.7821 - val_mse: 23.7821\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 29.0151 - mse: 29.0151 - val_loss: 23.3776 - val_mse: 23.3776\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 27.9319 - mse: 27.9319 - val_loss: 22.8592 - val_mse: 22.8592\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 26.7801 - mse: 26.7801 - val_loss: 22.0655 - val_mse: 22.0655\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 26.0249 - mse: 26.0249 - val_loss: 22.1360 - val_mse: 22.1360\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 25.4006 - mse: 25.4006 - val_loss: 21.8653 - val_mse: 21.8653\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 24.8677 - mse: 24.8677 - val_loss: 21.4873 - val_mse: 21.4873\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 24.4277 - mse: 24.4277 - val_loss: 20.9938 - val_mse: 20.9938\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 23.9572 - mse: 23.9572 - val_loss: 20.9256 - val_mse: 20.9256\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 23.5060 - mse: 23.5060 - val_loss: 20.6390 - val_mse: 20.6390\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 22.9981 - mse: 22.9981 - val_loss: 20.2100 - val_mse: 20.2100\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 22.6577 - mse: 22.6577 - val_loss: 20.3410 - val_mse: 20.3410\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 22.2645 - mse: 22.2645 - val_loss: 19.2571 - val_mse: 19.2571\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 21.8041 - mse: 21.8041 - val_loss: 19.7412 - val_mse: 19.7412\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 21.4191 - mse: 21.4191 - val_loss: 18.6960 - val_mse: 18.6960\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 21.0580 - mse: 21.0580 - val_loss: 18.4356 - val_mse: 18.4356\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 20.6601 - mse: 20.6601 - val_loss: 18.1640 - val_mse: 18.1640\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 20.3359 - mse: 20.3359 - val_loss: 17.4258 - val_mse: 17.4258\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 19.9671 - mse: 19.9671 - val_loss: 18.2432 - val_mse: 18.2432\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 19.6164 - mse: 19.6164 - val_loss: 17.1307 - val_mse: 17.1307\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 19.2453 - mse: 19.2453 - val_loss: 16.6567 - val_mse: 16.6567\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 18.8931 - mse: 18.8931 - val_loss: 16.6762 - val_mse: 16.6762\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 18.5091 - mse: 18.5091 - val_loss: 16.4989 - val_mse: 16.4989\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 18.2155 - mse: 18.2155 - val_loss: 15.7103 - val_mse: 15.7103\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 17.9891 - mse: 17.9891 - val_loss: 15.9941 - val_mse: 15.9941\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 17.7700 - mse: 17.7700 - val_loss: 15.7732 - val_mse: 15.7732\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 17.3303 - mse: 17.3303 - val_loss: 15.5674 - val_mse: 15.5674\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 16.9946 - mse: 16.9946 - val_loss: 14.9509 - val_mse: 14.9509\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 16.6159 - mse: 16.6159 - val_loss: 14.9523 - val_mse: 14.9523\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 16.3950 - mse: 16.3950 - val_loss: 15.0203 - val_mse: 15.0203\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 16.1009 - mse: 16.1009 - val_loss: 14.1444 - val_mse: 14.1444\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 15.7824 - mse: 15.7824 - val_loss: 14.0282 - val_mse: 14.0282\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 15.4985 - mse: 15.4985 - val_loss: 14.2123 - val_mse: 14.2123\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 15.2599 - mse: 15.2599 - val_loss: 14.0619 - val_mse: 14.0619\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 15.0062 - mse: 15.0062 - val_loss: 13.9896 - val_mse: 13.9896\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 14.6627 - mse: 14.6627 - val_loss: 13.1123 - val_mse: 13.1123\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 14.5056 - mse: 14.5056 - val_loss: 12.7511 - val_mse: 12.7511\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 14.0623 - mse: 14.0623 - val_loss: 13.0387 - val_mse: 13.0387\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13.8601 - mse: 13.8601 - val_loss: 12.7454 - val_mse: 12.7454\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 13.5300 - mse: 13.5300 - val_loss: 12.1919 - val_mse: 12.1919\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13.2473 - mse: 13.2473 - val_loss: 12.3280 - val_mse: 12.3280\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 13.0159 - mse: 13.0159 - val_loss: 12.0665 - val_mse: 12.0665\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.7730 - mse: 12.7730 - val_loss: 11.5429 - val_mse: 11.5429\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.4821 - mse: 12.4821 - val_loss: 11.6370 - val_mse: 11.6370\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.3425 - mse: 12.3425 - val_loss: 11.3431 - val_mse: 11.3431\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 12.1370 - mse: 12.1370 - val_loss: 11.1940 - val_mse: 11.1940\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 11.9245 - mse: 11.9245 - val_loss: 10.9901 - val_mse: 10.9901\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.7277 - mse: 11.7277 - val_loss: 10.9855 - val_mse: 10.9855\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.5654 - mse: 11.5654 - val_loss: 11.2610 - val_mse: 11.2610\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.3637 - mse: 11.3637 - val_loss: 10.6950 - val_mse: 10.6950\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.2433 - mse: 11.2433 - val_loss: 10.8303 - val_mse: 10.8303\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.0945 - mse: 11.0945 - val_loss: 10.6563 - val_mse: 10.6563\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.8884 - mse: 10.8884 - val_loss: 10.5426 - val_mse: 10.5426\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.8162 - mse: 10.8162 - val_loss: 10.5030 - val_mse: 10.5030\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.6597 - mse: 10.6597 - val_loss: 10.3459 - val_mse: 10.3459\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.5347 - mse: 10.5347 - val_loss: 10.2304 - val_mse: 10.2304\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.4424 - mse: 10.4424 - val_loss: 10.2121 - val_mse: 10.2121\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.3381 - mse: 10.3381 - val_loss: 10.0609 - val_mse: 10.0609\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.2045 - mse: 10.2045 - val_loss: 10.0274 - val_mse: 10.0274\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.1368 - mse: 10.1368 - val_loss: 9.9398 - val_mse: 9.9398\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.1115 - mse: 10.1115 - val_loss: 9.8413 - val_mse: 9.8413\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.9211 - mse: 9.9211 - val_loss: 9.9086 - val_mse: 9.9086\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.8415 - mse: 9.8415 - val_loss: 9.7159 - val_mse: 9.7159\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.7374 - mse: 9.7374 - val_loss: 9.7553 - val_mse: 9.7553\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.7931 - mse: 9.7931 - val_loss: 9.6540 - val_mse: 9.6540\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.6824 - mse: 9.6824 - val_loss: 9.7817 - val_mse: 9.7817\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.5577 - mse: 9.5577 - val_loss: 9.5033 - val_mse: 9.5033\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4985 - mse: 9.4985 - val_loss: 9.4408 - val_mse: 9.4408\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.4501 - mse: 9.4501 - val_loss: 9.6120 - val_mse: 9.6120\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 9.3352 - mse: 9.3352 - val_loss: 9.1992 - val_mse: 9.1992\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 9.2369 - mse: 9.2369 - val_loss: 9.3722 - val_mse: 9.3722\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 9.1492 - mse: 9.1492 - val_loss: 9.2630 - val_mse: 9.2630\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 9.1346 - mse: 9.1346 - val_loss: 9.0610 - val_mse: 9.0610\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 9.0039 - mse: 9.0039 - val_loss: 9.0671 - val_mse: 9.0671\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9345 - mse: 8.9345 - val_loss: 9.1389 - val_mse: 9.1389\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8707 - mse: 8.8707 - val_loss: 9.0389 - val_mse: 9.0389\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.8577 - mse: 8.8577 - val_loss: 9.0470 - val_mse: 9.0470\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7340 - mse: 8.7340 - val_loss: 9.0125 - val_mse: 9.0125\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.7865 - mse: 8.7865 - val_loss: 8.9282 - val_mse: 8.9282\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6302 - mse: 8.6302 - val_loss: 8.7257 - val_mse: 8.7257\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6577 - mse: 8.6577 - val_loss: 8.7517 - val_mse: 8.7517\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.5639 - mse: 8.5639 - val_loss: 8.8023 - val_mse: 8.8023\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.4041 - mse: 8.4041 - val_loss: 8.6111 - val_mse: 8.6111\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.4206 - mse: 8.4206 - val_loss: 8.6488 - val_mse: 8.6488\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.3380 - mse: 8.3380 - val_loss: 8.5799 - val_mse: 8.5799\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.2580 - mse: 8.2580 - val_loss: 8.5990 - val_mse: 8.5990\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2078 - mse: 8.2078 - val_loss: 8.4934 - val_mse: 8.4934\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2132 - mse: 8.2132 - val_loss: 8.5937 - val_mse: 8.5937\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.1039 - mse: 8.1039 - val_loss: 8.5881 - val_mse: 8.5881\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.0359 - mse: 8.0359 - val_loss: 8.3313 - val_mse: 8.3313\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0721 - mse: 8.0721 - val_loss: 8.3111 - val_mse: 8.3111\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.9444 - mse: 7.9444 - val_loss: 8.2692 - val_mse: 8.2692\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.9937 - mse: 7.9937 - val_loss: 8.2854 - val_mse: 8.2854\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8216 - mse: 7.8216 - val_loss: 8.1816 - val_mse: 8.1816\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7968 - mse: 7.7968 - val_loss: 8.2807 - val_mse: 8.2807\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8006 - mse: 7.8006 - val_loss: 8.2289 - val_mse: 8.2289\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6989 - mse: 7.6989 - val_loss: 8.1370 - val_mse: 8.1370\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.7311 - mse: 7.7311 - val_loss: 8.1963 - val_mse: 8.1963\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6501 - mse: 7.6501 - val_loss: 8.1684 - val_mse: 8.1684\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.6040 - mse: 7.6040 - val_loss: 8.2710 - val_mse: 8.2710\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5548 - mse: 7.5548 - val_loss: 8.3927 - val_mse: 8.3927\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6144 - mse: 7.6144 - val_loss: 8.1412 - val_mse: 8.1412\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4632 - mse: 7.4632 - val_loss: 8.1726 - val_mse: 8.1726\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4223 - mse: 7.4223 - val_loss: 8.0249 - val_mse: 8.0249\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4032 - mse: 7.4032 - val_loss: 8.0740 - val_mse: 8.0740\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.3578 - mse: 7.3578 - val_loss: 8.0322 - val_mse: 8.0322\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.3875 - mse: 7.3875 - val_loss: 8.1163 - val_mse: 8.1163\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2753 - mse: 7.2753 - val_loss: 7.9545 - val_mse: 7.9545\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.2457 - mse: 7.2457 - val_loss: 8.0345 - val_mse: 8.0345\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2577 - mse: 7.2577 - val_loss: 8.0387 - val_mse: 8.0387\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.2720 - mse: 7.2720 - val_loss: 8.0636 - val_mse: 8.0636\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1046 - mse: 7.1046 - val_loss: 7.9774 - val_mse: 7.9774\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1241 - mse: 7.1241 - val_loss: 7.9152 - val_mse: 7.9152\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1129 - mse: 7.1129 - val_loss: 7.9403 - val_mse: 7.9403\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0463 - mse: 7.0463 - val_loss: 7.9247 - val_mse: 7.9247\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0094 - mse: 7.0094 - val_loss: 7.8735 - val_mse: 7.8735\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9450 - mse: 6.9450 - val_loss: 7.9751 - val_mse: 7.9751\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8958 - mse: 6.8958 - val_loss: 7.7972 - val_mse: 7.7972\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8928 - mse: 6.8928 - val_loss: 7.8379 - val_mse: 7.8379\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8763 - mse: 6.8763 - val_loss: 7.8327 - val_mse: 7.8327\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8075 - mse: 6.8075 - val_loss: 7.8934 - val_mse: 7.8934\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8179 - mse: 6.8179 - val_loss: 7.8143 - val_mse: 7.8143\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7685 - mse: 6.7685 - val_loss: 7.8223 - val_mse: 7.8223\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8024 - mse: 6.8024 - val_loss: 7.8036 - val_mse: 7.8036\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.7258 - mse: 6.7258 - val_loss: 7.8258 - val_mse: 7.8258\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.6852 - mse: 6.6852 - val_loss: 7.7918 - val_mse: 7.7918\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.7233 - mse: 6.7233 - val_loss: 7.7642 - val_mse: 7.7642\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6295 - mse: 6.6295 - val_loss: 7.7158 - val_mse: 7.7158\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5894 - mse: 6.5894 - val_loss: 7.7476 - val_mse: 7.7476\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.5577 - mse: 6.5577 - val_loss: 7.6960 - val_mse: 7.6960\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.5152 - mse: 6.5152 - val_loss: 7.6576 - val_mse: 7.6576\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.5875 - mse: 6.5875 - val_loss: 7.6283 - val_mse: 7.6283\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.5310 - mse: 6.5310 - val_loss: 7.6871 - val_mse: 7.6871\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.4070 - mse: 6.4070 - val_loss: 7.7559 - val_mse: 7.7559\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.4662 - mse: 6.4662 - val_loss: 7.7100 - val_mse: 7.7100\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.3703 - mse: 6.3703 - val_loss: 7.6078 - val_mse: 7.6078\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.3726 - mse: 6.3726 - val_loss: 7.5656 - val_mse: 7.5656\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.2995 - mse: 6.2995 - val_loss: 7.5433 - val_mse: 7.5433\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.3618 - mse: 6.3618 - val_loss: 7.6493 - val_mse: 7.6493\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 6.2575 - mse: 6.2575 - val_loss: 7.5829 - val_mse: 7.5829\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.2605 - mse: 6.2605 - val_loss: 7.5116 - val_mse: 7.5116\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.2069 - mse: 6.2069 - val_loss: 7.6006 - val_mse: 7.6006\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.2641 - mse: 6.2641 - val_loss: 7.5680 - val_mse: 7.5680\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.2080 - mse: 6.2080 - val_loss: 7.5700 - val_mse: 7.5700\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.1058 - mse: 6.1058 - val_loss: 7.5770 - val_mse: 7.5770\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.0592 - mse: 6.0592 - val_loss: 7.5360 - val_mse: 7.5360\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.1041 - mse: 6.1041 - val_loss: 7.2156 - val_mse: 7.2156\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.0436 - mse: 6.0436 - val_loss: 7.1603 - val_mse: 7.1603\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.9779 - mse: 5.9779 - val_loss: 7.2254 - val_mse: 7.2254\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.0003 - mse: 6.0003 - val_loss: 7.2060 - val_mse: 7.2060\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 5.9447 - mse: 5.9447 - val_loss: 7.2046 - val_mse: 7.2046\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.9059 - mse: 5.9059 - val_loss: 7.2833 - val_mse: 7.2833\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 5.9446 - mse: 5.9446 - val_loss: 7.2162 - val_mse: 7.2162\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5.9631 - mse: 5.9631 - val_loss: 7.1557 - val_mse: 7.1557\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 5.8366 - mse: 5.8366 - val_loss: 7.2273 - val_mse: 7.2273\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.8095 - mse: 5.8095 - val_loss: 7.3342 - val_mse: 7.3342\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 5.9713 - mse: 5.9713 - val_loss: 7.2395 - val_mse: 7.2395\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.1891 - mse: 6.1891 - val_loss: 7.2297 - val_mse: 7.2297\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 5.9601 - mse: 5.9601 - val_loss: 7.2054 - val_mse: 7.2054\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 5.8294 - mse: 5.8294 - val_loss: 7.3330 - val_mse: 7.3330\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.7491 - mse: 5.7491 - val_loss: 7.2039 - val_mse: 7.2039\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 5.7084 - mse: 5.7084 - val_loss: 7.2575 - val_mse: 7.2575\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 5.7032 - mse: 5.7032 - val_loss: 7.1358 - val_mse: 7.1358\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5.6465 - mse: 5.6465 - val_loss: 7.0089 - val_mse: 7.0089\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.6125 - mse: 5.6125 - val_loss: 7.1549 - val_mse: 7.1549\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 5.6328 - mse: 5.6328 - val_loss: 7.1502 - val_mse: 7.1502\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 5.7705 - mse: 5.7705 - val_loss: 7.2013 - val_mse: 7.2013\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 5.7783 - mse: 5.7783 - val_loss: 7.1641 - val_mse: 7.1641\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5.6705 - mse: 5.6705 - val_loss: 7.0694 - val_mse: 7.0694\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.6816 - mse: 5.6816 - val_loss: 7.1672 - val_mse: 7.1672\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.5243 - mse: 5.5243 - val_loss: 7.0029 - val_mse: 7.0029\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 5.5253 - mse: 5.5253 - val_loss: 7.0678 - val_mse: 7.0678\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5.4884 - mse: 5.4884 - val_loss: 7.1038 - val_mse: 7.1038\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5.4727 - mse: 5.4727 - val_loss: 7.0724 - val_mse: 7.0724\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 5.5244 - mse: 5.5244 - val_loss: 7.0682 - val_mse: 7.0682\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5.4456 - mse: 5.4456 - val_loss: 7.0741 - val_mse: 7.0741\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 5.4368 - mse: 5.4368 - val_loss: 7.0998 - val_mse: 7.0998\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5.3768 - mse: 5.3768 - val_loss: 7.0543 - val_mse: 7.0543\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5.3521 - mse: 5.3521 - val_loss: 7.0482 - val_mse: 7.0482\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.3411 - mse: 5.3411 - val_loss: 7.0608 - val_mse: 7.0608\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 5.3714 - mse: 5.3714 - val_loss: 7.0044 - val_mse: 7.0044\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5.3240 - mse: 5.3240 - val_loss: 6.9993 - val_mse: 6.9993\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 5.3238 - mse: 5.3238 - val_loss: 6.9932 - val_mse: 6.9932\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 5.3039 - mse: 5.3039 - val_loss: 6.9984 - val_mse: 6.9984\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 5.2687 - mse: 5.2687 - val_loss: 7.0516 - val_mse: 7.0516\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5.2900 - mse: 5.2900 - val_loss: 6.9352 - val_mse: 6.9352\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.2832 - mse: 5.2832 - val_loss: 6.9793 - val_mse: 6.9793\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.3266 - mse: 5.3266 - val_loss: 6.8989 - val_mse: 6.8989\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.2057 - mse: 5.2057 - val_loss: 7.0644 - val_mse: 7.0644\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.2751 - mse: 5.2751 - val_loss: 6.9054 - val_mse: 6.9054\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.2628 - mse: 5.2628 - val_loss: 6.8548 - val_mse: 6.8548\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 5.2442 - mse: 5.2442 - val_loss: 6.9659 - val_mse: 6.9659\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.1540 - mse: 5.1540 - val_loss: 6.8859 - val_mse: 6.8859\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.1661 - mse: 5.1661 - val_loss: 6.9174 - val_mse: 6.9174\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 5.1395 - mse: 5.1395 - val_loss: 6.8039 - val_mse: 6.8039\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5.1451 - mse: 5.1451 - val_loss: 6.8151 - val_mse: 6.8151\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 5.2584 - mse: 5.2584 - val_loss: 6.9446 - val_mse: 6.9446\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5.2005 - mse: 5.2005 - val_loss: 6.8240 - val_mse: 6.8240\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.2167 - mse: 5.2167 - val_loss: 6.9651 - val_mse: 6.9651\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 5.3494 - mse: 5.3494 - val_loss: 6.9540 - val_mse: 6.9540\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 5.1056 - mse: 5.1056 - val_loss: 6.9675 - val_mse: 6.9675\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5.0631 - mse: 5.0631 - val_loss: 6.9879 - val_mse: 6.9879\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.0794 - mse: 5.0794 - val_loss: 6.9371 - val_mse: 6.9371\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.1123 - mse: 5.1123 - val_loss: 6.8714 - val_mse: 6.8714\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.9860 - mse: 4.9860 - val_loss: 6.8083 - val_mse: 6.8083\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.9403 - mse: 4.9403 - val_loss: 6.9041 - val_mse: 6.9041\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.9715 - mse: 4.9715 - val_loss: 6.8146 - val_mse: 6.8146\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.9623 - mse: 4.9623 - val_loss: 7.0599 - val_mse: 7.0599\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.0376 - mse: 5.0376 - val_loss: 6.7790 - val_mse: 6.7790\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.9575 - mse: 4.9575 - val_loss: 6.7203 - val_mse: 6.7203\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.9170 - mse: 4.9170 - val_loss: 6.7702 - val_mse: 6.7702\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.8804 - mse: 4.8804 - val_loss: 6.8130 - val_mse: 6.8130\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.8993 - mse: 4.8993 - val_loss: 6.7830 - val_mse: 6.7830\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.9119 - mse: 4.9119 - val_loss: 6.6279 - val_mse: 6.6279\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8863 - mse: 4.8863 - val_loss: 6.7613 - val_mse: 6.7613\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.8648 - mse: 4.8648 - val_loss: 6.7159 - val_mse: 6.7159\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.8020 - mse: 4.8020 - val_loss: 6.6596 - val_mse: 6.6596\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.9759 - mse: 4.9759 - val_loss: 6.6223 - val_mse: 6.6223\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.8689 - mse: 4.8689 - val_loss: 6.6869 - val_mse: 6.6869\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.7964 - mse: 4.7964 - val_loss: 6.7625 - val_mse: 6.7625\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.8002 - mse: 4.8002 - val_loss: 6.6969 - val_mse: 6.6969\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.7760 - mse: 4.7760 - val_loss: 6.7765 - val_mse: 6.7765\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.7354 - mse: 4.7354 - val_loss: 6.6480 - val_mse: 6.6480\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.7201 - mse: 4.7201 - val_loss: 6.6213 - val_mse: 6.6213\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.7145 - mse: 4.7145 - val_loss: 6.6156 - val_mse: 6.6156\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.7389 - mse: 4.7389 - val_loss: 6.6091 - val_mse: 6.6091\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.7246 - mse: 4.7246 - val_loss: 6.6339 - val_mse: 6.6339\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.7279 - mse: 4.7279 - val_loss: 6.6199 - val_mse: 6.6199\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.7820 - mse: 4.7820 - val_loss: 6.6069 - val_mse: 6.6069\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.7448 - mse: 4.7448 - val_loss: 6.6868 - val_mse: 6.6868\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.7054 - mse: 4.7054 - val_loss: 6.5990 - val_mse: 6.5990\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.7077 - mse: 4.7077 - val_loss: 6.5486 - val_mse: 6.5486\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.7420 - mse: 4.7420 - val_loss: 6.5489 - val_mse: 6.5489\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd9d0f852b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define errors\n",
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "# Define mean squared error function\n",
        "def mse():\n",
        "    return (errors ** 2).mean()\n",
        "\n",
        "# Compute mean squared errors for each error value\n",
        "mse_values = [mse() for error in errors]\n",
        "\n",
        "# Plot MSE values\n",
        "plt.plot(errors, mse_values, c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated outputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "soZYaRVgO1tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define errors\n",
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "# Define mean absolute error function\n",
        "def mae():\n",
        "    return np.abs(errors).mean()\n",
        "\n",
        "# Compute mean absolute errors for each error value\n",
        "mae_values = [mae() for error in errors]\n",
        "\n",
        "# Plot MAE values\n",
        "plt.plot(errors, mae_values, c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated outputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XSmVGe5kOsuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define actual and estimated outputs\n",
        "actual_outputs = np.arange(0, 51)\n",
        "n = len(actual_outputs)\n",
        "estimated_outputs = np.zeros(n)\n",
        "\n",
        "# Define mean squared logarithmic error function\n",
        "def msle():\n",
        "    return ((np.log(1 + actual_outputs) - np.log(1 + estimated_outputs)) ** 2).mean()\n",
        "\n",
        "# Compute mean squared logarithmic errors for each actual output value\n",
        "msle_values = [msle() for actual_output in actual_outputs]\n",
        "\n",
        "# Plot MSLE values\n",
        "plt.plot(actual_outputs, msle_values, c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual outputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "A4mAe56_MU8u",
        "outputId": "25c3447f-3cfe-46a7-f2f6-fd462105e0ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX5w42vRXMWj"
      },
      "source": [
        "### Answer 4\n",
        "\n",
        "To avoid the logarithm of zero, which is undefined. The formula for mean squared logarithmic error is:\n",
        "\n",
        "$$MSLE = \\frac 1{n}{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}$$ \n",
        " \n",
        "Here, $y_i$ is the actual output, and $\\hat{y}_i$ is the estimated output. By adding 1 to both $y_i$ and $\\hat{y}_i$, we ensure that the logarithm is only taken of positive values, and we avoid taking the logarithm of zero. This helps to make the mean squared logarithmic error more well-defined and numerically stable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOUf7iOkYxcM"
      },
      "source": [
        "### Answer 5\n",
        "\n",
        "Mean squared error (MSE), mean absolute error (MAE), and mean squared logarithmic error (MSLE) are three common loss functions used in regression problems. Here are some observations and comparisons:\n",
        "\n",
        "* MSE measures the average squared difference between the actual and predicted values. It penalizes larger errors more heavily than smaller errors. It is sensitive to outliers and can be influenced by large errors.\n",
        "\n",
        "* MAE measures the average absolute difference between the actual and predicted values. It gives equal weight to all errors, regardless of their size. It is less sensitive to outliers than MSE.\n",
        "\n",
        "* MSLE measures the average squared logarithmic difference between the actual and predicted values. It is often used when the data has a wide range of values or when the errors are expected to be proportional to the size of the actual value. It penalizes underestimation more heavily than overestimation.\n",
        "\n",
        "When comparing the results achieved with all three loss functions, it is important to consider the specific problem and the characteristics of the data. In some cases, one loss function may be more appropriate than the others. For example, MSE may be appropriate when the data has a normal distribution, while MAE may be better for data with heavy-tailed distributions. MSLE may be suitable when the errors are expected to be proportional to the actual values.\n",
        "\n",
        "It's a good idea to try multiple loss functions and compare their results to determine the most appropriate one for a specific problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_9nfLOffJ_P"
      },
      "source": [
        "### Answer 6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mape',\n",
        "              metrics=[\"mape\", \"msle\"])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNYFgm4TUeBc",
        "outputId": "9d3703b2-32d9-4719-bf75-28d9407076f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 2s 20ms/step - loss: 98.4612 - mape: 98.4612 - msle: 8.2752 - val_loss: 94.5954 - val_mape: 94.5954 - val_msle: 5.9275\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 91.5593 - mape: 91.5593 - msle: 5.0643 - val_loss: 86.6796 - val_mape: 86.6796 - val_msle: 3.7105\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 82.2171 - mape: 82.2171 - msle: 3.0853 - val_loss: 75.2720 - val_mape: 75.2720 - val_msle: 2.0983\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 68.6003 - mape: 68.6003 - msle: 1.6721 - val_loss: 61.2005 - val_mape: 61.2005 - val_msle: 0.9962\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 54.5246 - mape: 54.5246 - msle: 0.7568 - val_loss: 45.0534 - val_mape: 45.0534 - val_msle: 0.3735\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 38.2064 - mape: 38.2064 - msle: 0.3054 - val_loss: 31.5428 - val_mape: 31.5428 - val_msle: 0.1362\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 30.4360 - mape: 30.4360 - msle: 0.1563 - val_loss: 26.6336 - val_mape: 26.6336 - val_msle: 0.0917\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 25.4680 - mape: 25.4680 - msle: 0.1124 - val_loss: 20.9164 - val_mape: 20.9164 - val_msle: 0.0650\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 20.7051 - mape: 20.7051 - msle: 0.0861 - val_loss: 17.5374 - val_mape: 17.5374 - val_msle: 0.0513\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 17.7911 - mape: 17.7911 - msle: 0.0651 - val_loss: 16.3974 - val_mape: 16.3974 - val_msle: 0.0424\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 15.9834 - mape: 15.9834 - msle: 0.0523 - val_loss: 16.3480 - val_mape: 16.3480 - val_msle: 0.0452\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 14.9581 - mape: 14.9581 - msle: 0.0489 - val_loss: 16.1646 - val_mape: 16.1646 - val_msle: 0.0445\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 14.4369 - mape: 14.4369 - msle: 0.0424 - val_loss: 15.4044 - val_mape: 15.4044 - val_msle: 0.0431\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 14.0152 - mape: 14.0152 - msle: 0.0422 - val_loss: 14.6931 - val_mape: 14.6931 - val_msle: 0.0372\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 13.3004 - mape: 13.3004 - msle: 0.0371 - val_loss: 15.2242 - val_mape: 15.2242 - val_msle: 0.0408\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.9577 - mape: 12.9577 - msle: 0.0379 - val_loss: 14.1721 - val_mape: 14.1721 - val_msle: 0.0333\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 12.6918 - mape: 12.6918 - msle: 0.0323 - val_loss: 14.6917 - val_mape: 14.6917 - val_msle: 0.0371\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.3423 - mape: 12.3423 - msle: 0.0354 - val_loss: 13.8435 - val_mape: 13.8435 - val_msle: 0.0313\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 11.6661 - mape: 11.6661 - msle: 0.0304 - val_loss: 13.8095 - val_mape: 13.8095 - val_msle: 0.0312\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 11.4818 - mape: 11.4818 - msle: 0.0304 - val_loss: 13.6190 - val_mape: 13.6190 - val_msle: 0.0298\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 11.2918 - mape: 11.2918 - msle: 0.0310 - val_loss: 13.0490 - val_mape: 13.0490 - val_msle: 0.0267\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 11.0041 - mape: 11.0041 - msle: 0.0269 - val_loss: 13.2259 - val_mape: 13.2259 - val_msle: 0.0292\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10.7874 - mape: 10.7874 - msle: 0.0278 - val_loss: 13.3203 - val_mape: 13.3203 - val_msle: 0.0264\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.6118 - mape: 10.6118 - msle: 0.0279 - val_loss: 12.2854 - val_mape: 12.2854 - val_msle: 0.0237\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.5924 - mape: 10.5924 - msle: 0.0253 - val_loss: 13.7287 - val_mape: 13.7287 - val_msle: 0.0314\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 10.5019 - mape: 10.5019 - msle: 0.0281 - val_loss: 12.3225 - val_mape: 12.3225 - val_msle: 0.0237\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 10.3178 - mape: 10.3178 - msle: 0.0257 - val_loss: 12.5348 - val_mape: 12.5348 - val_msle: 0.0223\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10.1344 - mape: 10.1344 - msle: 0.0249 - val_loss: 13.2756 - val_mape: 13.2756 - val_msle: 0.0286\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.3169 - mape: 10.3169 - msle: 0.0251 - val_loss: 12.9583 - val_mape: 12.9583 - val_msle: 0.0249\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.1003 - mape: 10.1003 - msle: 0.0252 - val_loss: 12.0009 - val_mape: 12.0009 - val_msle: 0.0213\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.1865 - mape: 10.1865 - msle: 0.0244 - val_loss: 12.4496 - val_mape: 12.4496 - val_msle: 0.0241\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.9412 - mape: 9.9412 - msle: 0.0240 - val_loss: 12.1268 - val_mape: 12.1268 - val_msle: 0.0214\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.8594 - mape: 9.8594 - msle: 0.0243 - val_loss: 11.4271 - val_mape: 11.4271 - val_msle: 0.0188\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.8876 - mape: 9.8876 - msle: 0.0229 - val_loss: 12.6525 - val_mape: 12.6525 - val_msle: 0.0252\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.8435 - mape: 9.8435 - msle: 0.0228 - val_loss: 11.1960 - val_mape: 11.1960 - val_msle: 0.0187\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.5032 - mape: 9.5032 - msle: 0.0241 - val_loss: 12.0712 - val_mape: 12.0712 - val_msle: 0.0214\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.4097 - mape: 9.4097 - msle: 0.0226 - val_loss: 12.0113 - val_mape: 12.0113 - val_msle: 0.0224\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.4004 - mape: 9.4004 - msle: 0.0225 - val_loss: 11.7857 - val_mape: 11.7857 - val_msle: 0.0217\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.2453 - mape: 9.2453 - msle: 0.0216 - val_loss: 11.6810 - val_mape: 11.6810 - val_msle: 0.0211\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 9.1395 - mape: 9.1395 - msle: 0.0219 - val_loss: 11.1044 - val_mape: 11.1044 - val_msle: 0.0175\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.2400 - mape: 9.2400 - msle: 0.0216 - val_loss: 12.1995 - val_mape: 12.1995 - val_msle: 0.0240\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.5500 - mape: 9.5500 - msle: 0.0227 - val_loss: 11.1111 - val_mape: 11.1111 - val_msle: 0.0183\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.2418 - mape: 9.2418 - msle: 0.0212 - val_loss: 12.1824 - val_mape: 12.1824 - val_msle: 0.0209\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.0225 - mape: 9.0225 - msle: 0.0205 - val_loss: 11.6314 - val_mape: 11.6314 - val_msle: 0.0209\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9447 - mape: 8.9447 - msle: 0.0206 - val_loss: 11.5071 - val_mape: 11.5071 - val_msle: 0.0198\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.7974 - mape: 8.7974 - msle: 0.0207 - val_loss: 11.1824 - val_mape: 11.1824 - val_msle: 0.0187\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.7564 - mape: 8.7564 - msle: 0.0207 - val_loss: 11.4603 - val_mape: 11.4603 - val_msle: 0.0200\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.7643 - mape: 8.7643 - msle: 0.0200 - val_loss: 11.5694 - val_mape: 11.5694 - val_msle: 0.0204\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7021 - mape: 8.7021 - msle: 0.0198 - val_loss: 11.1748 - val_mape: 11.1748 - val_msle: 0.0184\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.7760 - mape: 8.7760 - msle: 0.0206 - val_loss: 11.7056 - val_mape: 11.7056 - val_msle: 0.0197\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8149 - mape: 8.8149 - msle: 0.0200 - val_loss: 11.4869 - val_mape: 11.4869 - val_msle: 0.0202\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.6186 - mape: 8.6186 - msle: 0.0198 - val_loss: 11.5899 - val_mape: 11.5899 - val_msle: 0.0208\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.5654 - mape: 8.5654 - msle: 0.0190 - val_loss: 11.2275 - val_mape: 11.2275 - val_msle: 0.0189\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.5305 - mape: 8.5305 - msle: 0.0204 - val_loss: 11.3886 - val_mape: 11.3886 - val_msle: 0.0192\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.4927 - mape: 8.4927 - msle: 0.0187 - val_loss: 11.4241 - val_mape: 11.4241 - val_msle: 0.0195\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.6079 - mape: 8.6079 - msle: 0.0191 - val_loss: 11.0346 - val_mape: 11.0346 - val_msle: 0.0182\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.5239 - mape: 8.5239 - msle: 0.0193 - val_loss: 11.0204 - val_mape: 11.0204 - val_msle: 0.0178\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.3136 - mape: 8.3136 - msle: 0.0189 - val_loss: 11.0953 - val_mape: 11.0953 - val_msle: 0.0182\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2793 - mape: 8.2793 - msle: 0.0187 - val_loss: 11.1457 - val_mape: 11.1457 - val_msle: 0.0187\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.3877 - mape: 8.3877 - msle: 0.0190 - val_loss: 10.8612 - val_mape: 10.8612 - val_msle: 0.0162\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.4818 - mape: 8.4818 - msle: 0.0196 - val_loss: 10.8174 - val_mape: 10.8174 - val_msle: 0.0155\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.4677 - mape: 8.4677 - msle: 0.0177 - val_loss: 11.4717 - val_mape: 11.4717 - val_msle: 0.0198\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2002 - mape: 8.2002 - msle: 0.0190 - val_loss: 10.5536 - val_mape: 10.5536 - val_msle: 0.0154\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.1927 - mape: 8.1927 - msle: 0.0182 - val_loss: 11.2402 - val_mape: 11.2402 - val_msle: 0.0195\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.1341 - mape: 8.1341 - msle: 0.0185 - val_loss: 10.8348 - val_mape: 10.8348 - val_msle: 0.0171\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.1138 - mape: 8.1138 - msle: 0.0186 - val_loss: 10.7821 - val_mape: 10.7821 - val_msle: 0.0164\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.0618 - mape: 8.0618 - msle: 0.0179 - val_loss: 10.8754 - val_mape: 10.8754 - val_msle: 0.0170\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.0328 - mape: 8.0328 - msle: 0.0183 - val_loss: 10.5176 - val_mape: 10.5176 - val_msle: 0.0151\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0659 - mape: 8.0659 - msle: 0.0183 - val_loss: 10.7313 - val_mape: 10.7313 - val_msle: 0.0162\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.1344 - mape: 8.1344 - msle: 0.0188 - val_loss: 11.0397 - val_mape: 11.0397 - val_msle: 0.0158\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.3000 - mape: 8.3000 - msle: 0.0180 - val_loss: 10.9176 - val_mape: 10.9176 - val_msle: 0.0176\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.9409 - mape: 7.9409 - msle: 0.0172 - val_loss: 10.9220 - val_mape: 10.9220 - val_msle: 0.0172\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8478 - mape: 7.8478 - msle: 0.0174 - val_loss: 10.5583 - val_mape: 10.5583 - val_msle: 0.0156\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.8404 - mape: 7.8404 - msle: 0.0172 - val_loss: 10.7122 - val_mape: 10.7122 - val_msle: 0.0161\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8429 - mape: 7.8429 - msle: 0.0175 - val_loss: 10.5455 - val_mape: 10.5455 - val_msle: 0.0153\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7624 - mape: 7.7624 - msle: 0.0167 - val_loss: 11.1020 - val_mape: 11.1020 - val_msle: 0.0180\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.1607 - mape: 8.1607 - msle: 0.0181 - val_loss: 10.3325 - val_mape: 10.3325 - val_msle: 0.0144\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.1493 - mape: 8.1493 - msle: 0.0172 - val_loss: 11.0761 - val_mape: 11.0761 - val_msle: 0.0175\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.9279 - mape: 7.9279 - msle: 0.0172 - val_loss: 10.3796 - val_mape: 10.3796 - val_msle: 0.0150\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8192 - mape: 7.8192 - msle: 0.0168 - val_loss: 10.7105 - val_mape: 10.7105 - val_msle: 0.0168\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7022 - mape: 7.7022 - msle: 0.0167 - val_loss: 10.8714 - val_mape: 10.8714 - val_msle: 0.0176\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.9926 - mape: 7.9926 - msle: 0.0176 - val_loss: 10.9153 - val_mape: 10.9153 - val_msle: 0.0172\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.8503 - mape: 7.8503 - msle: 0.0170 - val_loss: 10.3728 - val_mape: 10.3728 - val_msle: 0.0142\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.5479 - mape: 7.5479 - msle: 0.0168 - val_loss: 10.3203 - val_mape: 10.3203 - val_msle: 0.0146\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6294 - mape: 7.6294 - msle: 0.0166 - val_loss: 10.6431 - val_mape: 10.6431 - val_msle: 0.0168\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5299 - mape: 7.5299 - msle: 0.0163 - val_loss: 10.7157 - val_mape: 10.7157 - val_msle: 0.0164\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.4952 - mape: 7.4952 - msle: 0.0158 - val_loss: 10.9387 - val_mape: 10.9387 - val_msle: 0.0180\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.9358 - mape: 7.9358 - msle: 0.0174 - val_loss: 10.3210 - val_mape: 10.3210 - val_msle: 0.0142\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.4845 - mape: 7.4845 - msle: 0.0163 - val_loss: 10.2533 - val_mape: 10.2533 - val_msle: 0.0144\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.5870 - mape: 7.5870 - msle: 0.0159 - val_loss: 10.5105 - val_mape: 10.5105 - val_msle: 0.0150\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.4592 - mape: 7.4592 - msle: 0.0158 - val_loss: 10.4091 - val_mape: 10.4091 - val_msle: 0.0146\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4876 - mape: 7.4876 - msle: 0.0161 - val_loss: 10.4192 - val_mape: 10.4192 - val_msle: 0.0148\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.5617 - mape: 7.5617 - msle: 0.0161 - val_loss: 9.9835 - val_mape: 9.9835 - val_msle: 0.0134\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.5832 - mape: 7.5832 - msle: 0.0166 - val_loss: 10.5084 - val_mape: 10.5084 - val_msle: 0.0156\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.6512 - mape: 7.6512 - msle: 0.0156 - val_loss: 10.7143 - val_mape: 10.7143 - val_msle: 0.0176\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.6381 - mape: 7.6381 - msle: 0.0160 - val_loss: 10.1133 - val_mape: 10.1133 - val_msle: 0.0140\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.4490 - mape: 7.4490 - msle: 0.0165 - val_loss: 10.1248 - val_mape: 10.1248 - val_msle: 0.0138\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.3034 - mape: 7.3034 - msle: 0.0158 - val_loss: 10.1871 - val_mape: 10.1871 - val_msle: 0.0145\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2119 - mape: 7.2119 - msle: 0.0153 - val_loss: 10.2498 - val_mape: 10.2498 - val_msle: 0.0149\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1450 - mape: 7.1450 - msle: 0.0156 - val_loss: 10.0889 - val_mape: 10.0889 - val_msle: 0.0142\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2645 - mape: 7.2645 - msle: 0.0158 - val_loss: 10.3475 - val_mape: 10.3475 - val_msle: 0.0153\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1348 - mape: 7.1348 - msle: 0.0156 - val_loss: 9.9365 - val_mape: 9.9365 - val_msle: 0.0134\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.2282 - mape: 7.2282 - msle: 0.0150 - val_loss: 10.5584 - val_mape: 10.5584 - val_msle: 0.0153\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1372 - mape: 7.1372 - msle: 0.0155 - val_loss: 10.1059 - val_mape: 10.1059 - val_msle: 0.0133\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.3155 - mape: 7.3155 - msle: 0.0150 - val_loss: 10.3414 - val_mape: 10.3414 - val_msle: 0.0159\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2376 - mape: 7.2376 - msle: 0.0151 - val_loss: 10.4898 - val_mape: 10.4898 - val_msle: 0.0165\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.0495 - mape: 7.0495 - msle: 0.0149 - val_loss: 10.0815 - val_mape: 10.0815 - val_msle: 0.0139\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1090 - mape: 7.1090 - msle: 0.0151 - val_loss: 10.2348 - val_mape: 10.2348 - val_msle: 0.0144\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.0005 - mape: 7.0005 - msle: 0.0154 - val_loss: 10.0712 - val_mape: 10.0712 - val_msle: 0.0143\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9398 - mape: 6.9398 - msle: 0.0151 - val_loss: 10.5554 - val_mape: 10.5554 - val_msle: 0.0159\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1176 - mape: 7.1176 - msle: 0.0145 - val_loss: 9.9626 - val_mape: 9.9626 - val_msle: 0.0134\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9807 - mape: 6.9807 - msle: 0.0149 - val_loss: 9.8732 - val_mape: 9.8732 - val_msle: 0.0134\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.0453 - mape: 7.0453 - msle: 0.0146 - val_loss: 10.4360 - val_mape: 10.4360 - val_msle: 0.0148\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9326 - mape: 6.9326 - msle: 0.0150 - val_loss: 9.8181 - val_mape: 9.8181 - val_msle: 0.0129\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0434 - mape: 7.0434 - msle: 0.0146 - val_loss: 10.0920 - val_mape: 10.0920 - val_msle: 0.0145\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0221 - mape: 7.0221 - msle: 0.0142 - val_loss: 10.4434 - val_mape: 10.4434 - val_msle: 0.0155\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1999 - mape: 7.1999 - msle: 0.0150 - val_loss: 10.1815 - val_mape: 10.1815 - val_msle: 0.0144\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.3388 - mape: 7.3388 - msle: 0.0163 - val_loss: 9.8866 - val_mape: 9.8866 - val_msle: 0.0135\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4435 - mape: 7.4435 - msle: 0.0146 - val_loss: 10.2086 - val_mape: 10.2086 - val_msle: 0.0151\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9735 - mape: 6.9735 - msle: 0.0148 - val_loss: 9.7554 - val_mape: 9.7554 - val_msle: 0.0131\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.1912 - mape: 7.1912 - msle: 0.0141 - val_loss: 10.4446 - val_mape: 10.4446 - val_msle: 0.0153\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9782 - mape: 6.9782 - msle: 0.0141 - val_loss: 10.2144 - val_mape: 10.2144 - val_msle: 0.0146\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9219 - mape: 6.9219 - msle: 0.0147 - val_loss: 10.2800 - val_mape: 10.2800 - val_msle: 0.0155\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0569 - mape: 7.0569 - msle: 0.0144 - val_loss: 10.3453 - val_mape: 10.3453 - val_msle: 0.0153\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.9185 - mape: 6.9185 - msle: 0.0143 - val_loss: 10.0114 - val_mape: 10.0114 - val_msle: 0.0133\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8835 - mape: 6.8835 - msle: 0.0136 - val_loss: 10.4310 - val_mape: 10.4310 - val_msle: 0.0158\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.8115 - mape: 6.8115 - msle: 0.0142 - val_loss: 10.2270 - val_mape: 10.2270 - val_msle: 0.0150\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 7.1779 - mape: 7.1779 - msle: 0.0151 - val_loss: 9.9032 - val_mape: 9.9032 - val_msle: 0.0135\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.8074 - mape: 6.8074 - msle: 0.0149 - val_loss: 9.7487 - val_mape: 9.7487 - val_msle: 0.0133\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.8898 - mape: 6.8898 - msle: 0.0138 - val_loss: 10.2247 - val_mape: 10.2247 - val_msle: 0.0144\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.7509 - mape: 6.7509 - msle: 0.0140 - val_loss: 9.9376 - val_mape: 9.9376 - val_msle: 0.0137\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 6.5043 - mape: 6.5043 - msle: 0.0140 - val_loss: 9.9213 - val_mape: 9.9213 - val_msle: 0.0138\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.5047 - mape: 6.5047 - msle: 0.0133 - val_loss: 10.4167 - val_mape: 10.4167 - val_msle: 0.0158\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 6.6248 - mape: 6.6248 - msle: 0.0140 - val_loss: 9.9496 - val_mape: 9.9496 - val_msle: 0.0137\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 6.7715 - mape: 6.7715 - msle: 0.0147 - val_loss: 9.7446 - val_mape: 9.7446 - val_msle: 0.0134\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 6.7262 - mape: 6.7262 - msle: 0.0132 - val_loss: 10.2883 - val_mape: 10.2883 - val_msle: 0.0158\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 7.0113 - mape: 7.0113 - msle: 0.0137 - val_loss: 10.0177 - val_mape: 10.0177 - val_msle: 0.0141\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 6.5989 - mape: 6.5989 - msle: 0.0143 - val_loss: 9.6959 - val_mape: 9.6959 - val_msle: 0.0134\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 6.5140 - mape: 6.5140 - msle: 0.0136 - val_loss: 10.0257 - val_mape: 10.0257 - val_msle: 0.0145\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.6321 - mape: 6.6321 - msle: 0.0137 - val_loss: 9.6183 - val_mape: 9.6183 - val_msle: 0.0130\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 6.4822 - mape: 6.4822 - msle: 0.0133 - val_loss: 9.9171 - val_mape: 9.9171 - val_msle: 0.0139\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.4713 - mape: 6.4713 - msle: 0.0138 - val_loss: 9.6917 - val_mape: 9.6917 - val_msle: 0.0133\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.4672 - mape: 6.4672 - msle: 0.0135 - val_loss: 9.8721 - val_mape: 9.8721 - val_msle: 0.0138\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 6.8166 - mape: 6.8166 - msle: 0.0137 - val_loss: 10.4577 - val_mape: 10.4577 - val_msle: 0.0163\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.6846 - mape: 6.6846 - msle: 0.0134 - val_loss: 10.2061 - val_mape: 10.2061 - val_msle: 0.0156\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.6194 - mape: 6.6194 - msle: 0.0140 - val_loss: 9.6328 - val_mape: 9.6328 - val_msle: 0.0130\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.4888 - mape: 6.4888 - msle: 0.0134 - val_loss: 9.9805 - val_mape: 9.9805 - val_msle: 0.0145\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 6.5712 - mape: 6.5712 - msle: 0.0131 - val_loss: 9.9734 - val_mape: 9.9734 - val_msle: 0.0144\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.4352 - mape: 6.4352 - msle: 0.0129 - val_loss: 10.3147 - val_mape: 10.3147 - val_msle: 0.0164\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.9291 - mape: 6.9291 - msle: 0.0144 - val_loss: 9.7718 - val_mape: 9.7718 - val_msle: 0.0139\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.4593 - mape: 6.4593 - msle: 0.0132 - val_loss: 9.8688 - val_mape: 9.8688 - val_msle: 0.0140\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.5404 - mape: 6.5404 - msle: 0.0135 - val_loss: 9.5663 - val_mape: 9.5663 - val_msle: 0.0131\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.4260 - mape: 6.4260 - msle: 0.0129 - val_loss: 9.5024 - val_mape: 9.5024 - val_msle: 0.0129\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.2428 - mape: 6.2428 - msle: 0.0128 - val_loss: 9.9795 - val_mape: 9.9795 - val_msle: 0.0141\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.0878 - mape: 6.0878 - msle: 0.0125 - val_loss: 9.6610 - val_mape: 9.6610 - val_msle: 0.0133\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.2534 - mape: 6.2534 - msle: 0.0126 - val_loss: 9.3359 - val_mape: 9.3359 - val_msle: 0.0128\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.2683 - mape: 6.2683 - msle: 0.0125 - val_loss: 9.9671 - val_mape: 9.9671 - val_msle: 0.0144\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.2673 - mape: 6.2673 - msle: 0.0125 - val_loss: 9.8808 - val_mape: 9.8808 - val_msle: 0.0143\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.4329 - mape: 6.4329 - msle: 0.0128 - val_loss: 9.6251 - val_mape: 9.6251 - val_msle: 0.0135\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.4386 - mape: 6.4386 - msle: 0.0135 - val_loss: 9.8319 - val_mape: 9.8319 - val_msle: 0.0141\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.1877 - mape: 6.1877 - msle: 0.0127 - val_loss: 9.5790 - val_mape: 9.5790 - val_msle: 0.0134\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 6.2819 - mape: 6.2819 - msle: 0.0129 - val_loss: 9.6236 - val_mape: 9.6236 - val_msle: 0.0132\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 6.1075 - mape: 6.1075 - msle: 0.0129 - val_loss: 9.5362 - val_mape: 9.5362 - val_msle: 0.0131\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.2343 - mape: 6.2343 - msle: 0.0131 - val_loss: 9.5292 - val_mape: 9.5292 - val_msle: 0.0132\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.5019 - mape: 6.5019 - msle: 0.0122 - val_loss: 10.1045 - val_mape: 10.1045 - val_msle: 0.0157\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.4240 - mape: 6.4240 - msle: 0.0137 - val_loss: 9.8414 - val_mape: 9.8414 - val_msle: 0.0141\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.2789 - mape: 6.2789 - msle: 0.0135 - val_loss: 9.3647 - val_mape: 9.3647 - val_msle: 0.0129\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.2286 - mape: 6.2286 - msle: 0.0126 - val_loss: 9.5921 - val_mape: 9.5921 - val_msle: 0.0137\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 6.1145 - mape: 6.1145 - msle: 0.0121 - val_loss: 9.8355 - val_mape: 9.8355 - val_msle: 0.0140\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 6.0325 - mape: 6.0325 - msle: 0.0121 - val_loss: 9.3288 - val_mape: 9.3288 - val_msle: 0.0127\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 6.3398 - mape: 6.3398 - msle: 0.0129 - val_loss: 9.7166 - val_mape: 9.7166 - val_msle: 0.0138\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.4349 - mape: 6.4349 - msle: 0.0127 - val_loss: 9.8327 - val_mape: 9.8327 - val_msle: 0.0145\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.1240 - mape: 6.1240 - msle: 0.0126 - val_loss: 9.5275 - val_mape: 9.5275 - val_msle: 0.0131\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.0567 - mape: 6.0567 - msle: 0.0122 - val_loss: 9.6852 - val_mape: 9.6852 - val_msle: 0.0135\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.1195 - mape: 6.1195 - msle: 0.0122 - val_loss: 9.6996 - val_mape: 9.6996 - val_msle: 0.0140\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.1117 - mape: 6.1117 - msle: 0.0120 - val_loss: 9.9149 - val_mape: 9.9149 - val_msle: 0.0146\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.1665 - mape: 6.1665 - msle: 0.0119 - val_loss: 9.7479 - val_mape: 9.7479 - val_msle: 0.0138\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 6.1423 - mape: 6.1423 - msle: 0.0126 - val_loss: 9.1441 - val_mape: 9.1441 - val_msle: 0.0123\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.1032 - mape: 6.1032 - msle: 0.0116 - val_loss: 9.3445 - val_mape: 9.3445 - val_msle: 0.0127\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.0860 - mape: 6.0860 - msle: 0.0122 - val_loss: 9.7663 - val_mape: 9.7663 - val_msle: 0.0139\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.0096 - mape: 6.0096 - msle: 0.0120 - val_loss: 9.8995 - val_mape: 9.8995 - val_msle: 0.0140\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.3425 - mape: 6.3425 - msle: 0.0123 - val_loss: 9.5567 - val_mape: 9.5567 - val_msle: 0.0138\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.1569 - mape: 6.1569 - msle: 0.0119 - val_loss: 9.8400 - val_mape: 9.8400 - val_msle: 0.0148\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 6.3852 - mape: 6.3852 - msle: 0.0124 - val_loss: 9.4998 - val_mape: 9.4998 - val_msle: 0.0136\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.0396 - mape: 6.0396 - msle: 0.0124 - val_loss: 9.3168 - val_mape: 9.3168 - val_msle: 0.0130\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 6.0683 - mape: 6.0683 - msle: 0.0119 - val_loss: 9.9120 - val_mape: 9.9120 - val_msle: 0.0151\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.9160 - mape: 5.9160 - msle: 0.0117 - val_loss: 9.5257 - val_mape: 9.5257 - val_msle: 0.0139\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5.7674 - mape: 5.7674 - msle: 0.0118 - val_loss: 9.4635 - val_mape: 9.4635 - val_msle: 0.0134\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 5.6823 - mape: 5.6823 - msle: 0.0118 - val_loss: 9.3172 - val_mape: 9.3172 - val_msle: 0.0134\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.0618 - mape: 6.0618 - msle: 0.0116 - val_loss: 9.3553 - val_mape: 9.3553 - val_msle: 0.0137\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.9223 - mape: 5.9223 - msle: 0.0118 - val_loss: 9.0964 - val_mape: 9.0964 - val_msle: 0.0122\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.1321 - mape: 6.1321 - msle: 0.0119 - val_loss: 9.3899 - val_mape: 9.3899 - val_msle: 0.0133\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 6.0406 - mape: 6.0406 - msle: 0.0118 - val_loss: 9.4079 - val_mape: 9.4079 - val_msle: 0.0127\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 5.7916 - mape: 5.7916 - msle: 0.0116 - val_loss: 9.6621 - val_mape: 9.6621 - val_msle: 0.0137\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 5.8911 - mape: 5.8911 - msle: 0.0111 - val_loss: 9.4133 - val_mape: 9.4133 - val_msle: 0.0135\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 6.1788 - mape: 6.1788 - msle: 0.0125 - val_loss: 9.2944 - val_mape: 9.2944 - val_msle: 0.0132\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 6.1880 - mape: 6.1880 - msle: 0.0122 - val_loss: 9.6574 - val_mape: 9.6574 - val_msle: 0.0139\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5.9067 - mape: 5.9067 - msle: 0.0115 - val_loss: 9.6519 - val_mape: 9.6519 - val_msle: 0.0138\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 6.1805 - mape: 6.1805 - msle: 0.0118 - val_loss: 9.4214 - val_mape: 9.4214 - val_msle: 0.0134\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.0975 - mape: 6.0975 - msle: 0.0112 - val_loss: 9.8160 - val_mape: 9.8160 - val_msle: 0.0145\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 5.8967 - mape: 5.8967 - msle: 0.0113 - val_loss: 9.5682 - val_mape: 9.5682 - val_msle: 0.0135\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 5.6332 - mape: 5.6332 - msle: 0.0111 - val_loss: 9.4992 - val_mape: 9.4992 - val_msle: 0.0137\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 5.7345 - mape: 5.7345 - msle: 0.0110 - val_loss: 9.2782 - val_mape: 9.2782 - val_msle: 0.0133\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.8819 - mape: 5.8819 - msle: 0.0109 - val_loss: 9.4817 - val_mape: 9.4817 - val_msle: 0.0137\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.7718 - mape: 5.7718 - msle: 0.0112 - val_loss: 9.4401 - val_mape: 9.4401 - val_msle: 0.0131\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5.5943 - mape: 5.5943 - msle: 0.0113 - val_loss: 9.1913 - val_mape: 9.1913 - val_msle: 0.0130\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.6647 - mape: 5.6647 - msle: 0.0110 - val_loss: 9.0928 - val_mape: 9.0928 - val_msle: 0.0128\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5.5460 - mape: 5.5460 - msle: 0.0108 - val_loss: 9.6668 - val_mape: 9.6668 - val_msle: 0.0137\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.6318 - mape: 5.6318 - msle: 0.0110 - val_loss: 9.1300 - val_mape: 9.1300 - val_msle: 0.0125\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5.5435 - mape: 5.5435 - msle: 0.0108 - val_loss: 9.0815 - val_mape: 9.0815 - val_msle: 0.0126\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.5997 - mape: 5.5997 - msle: 0.0109 - val_loss: 9.1292 - val_mape: 9.1292 - val_msle: 0.0129\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.5063 - mape: 5.5063 - msle: 0.0108 - val_loss: 9.3039 - val_mape: 9.3039 - val_msle: 0.0129\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.6085 - mape: 5.6085 - msle: 0.0112 - val_loss: 9.0493 - val_mape: 9.0493 - val_msle: 0.0125\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.6339 - mape: 5.6339 - msle: 0.0107 - val_loss: 9.1164 - val_mape: 9.1164 - val_msle: 0.0130\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.8516 - mape: 5.8516 - msle: 0.0109 - val_loss: 9.4336 - val_mape: 9.4336 - val_msle: 0.0135\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.5542 - mape: 5.5542 - msle: 0.0108 - val_loss: 9.2865 - val_mape: 9.2865 - val_msle: 0.0130\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.7931 - mape: 5.7931 - msle: 0.0107 - val_loss: 9.7877 - val_mape: 9.7877 - val_msle: 0.0141\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.6458 - mape: 5.6458 - msle: 0.0109 - val_loss: 9.1244 - val_mape: 9.1244 - val_msle: 0.0130\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.4742 - mape: 5.4742 - msle: 0.0104 - val_loss: 9.0482 - val_mape: 9.0482 - val_msle: 0.0128\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 5.4544 - mape: 5.4544 - msle: 0.0108 - val_loss: 9.2244 - val_mape: 9.2244 - val_msle: 0.0130\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 5.4444 - mape: 5.4444 - msle: 0.0104 - val_loss: 9.6851 - val_mape: 9.6851 - val_msle: 0.0140\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5359 - mape: 5.5359 - msle: 0.0102 - val_loss: 9.2149 - val_mape: 9.2149 - val_msle: 0.0131\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.3019 - mape: 5.3019 - msle: 0.0103 - val_loss: 9.0598 - val_mape: 9.0598 - val_msle: 0.0125\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.4121 - mape: 5.4121 - msle: 0.0103 - val_loss: 9.5119 - val_mape: 9.5119 - val_msle: 0.0141\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.5227 - mape: 5.5227 - msle: 0.0107 - val_loss: 9.4887 - val_mape: 9.4887 - val_msle: 0.0135\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.7355 - mape: 5.7355 - msle: 0.0102 - val_loss: 9.2532 - val_mape: 9.2532 - val_msle: 0.0131\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.6307 - mape: 5.6307 - msle: 0.0109 - val_loss: 9.3693 - val_mape: 9.3693 - val_msle: 0.0134\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.6245 - mape: 5.6245 - msle: 0.0103 - val_loss: 9.1861 - val_mape: 9.1861 - val_msle: 0.0132\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5556 - mape: 5.5556 - msle: 0.0104 - val_loss: 9.2896 - val_mape: 9.2896 - val_msle: 0.0129\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.6929 - mape: 5.6929 - msle: 0.0106 - val_loss: 9.2957 - val_mape: 9.2957 - val_msle: 0.0129\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.3940 - mape: 5.3940 - msle: 0.0104 - val_loss: 9.0746 - val_mape: 9.0746 - val_msle: 0.0128\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.3495 - mape: 5.3495 - msle: 0.0102 - val_loss: 9.5857 - val_mape: 9.5857 - val_msle: 0.0142\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.6956 - mape: 5.6956 - msle: 0.0100 - val_loss: 9.3309 - val_mape: 9.3309 - val_msle: 0.0144\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.3649 - mape: 5.3649 - msle: 0.0102 - val_loss: 8.9047 - val_mape: 8.9047 - val_msle: 0.0125\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.4046 - mape: 5.4046 - msle: 0.0103 - val_loss: 8.9131 - val_mape: 8.9131 - val_msle: 0.0128\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.3419 - mape: 5.3419 - msle: 0.0101 - val_loss: 9.1658 - val_mape: 9.1658 - val_msle: 0.0122\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 5.2813 - mape: 5.2813 - msle: 0.0101 - val_loss: 9.3064 - val_mape: 9.3064 - val_msle: 0.0134\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.9343 - mape: 5.9343 - msle: 0.0110 - val_loss: 9.2880 - val_mape: 9.2880 - val_msle: 0.0137\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.5305 - mape: 5.5305 - msle: 0.0098 - val_loss: 9.6113 - val_mape: 9.6113 - val_msle: 0.0144\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.6124 - mape: 5.6124 - msle: 0.0103 - val_loss: 8.8579 - val_mape: 8.8579 - val_msle: 0.0124\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5.2817 - mape: 5.2817 - msle: 0.0100 - val_loss: 9.0602 - val_mape: 9.0602 - val_msle: 0.0128\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.1472 - mape: 5.1472 - msle: 0.0099 - val_loss: 9.2861 - val_mape: 9.2861 - val_msle: 0.0130\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.2609 - mape: 5.2609 - msle: 0.0098 - val_loss: 8.8952 - val_mape: 8.8952 - val_msle: 0.0128\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.2004 - mape: 5.2004 - msle: 0.0097 - val_loss: 9.0521 - val_mape: 9.0521 - val_msle: 0.0137\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.3668 - mape: 5.3668 - msle: 0.0098 - val_loss: 9.1136 - val_mape: 9.1136 - val_msle: 0.0140\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.1006 - mape: 5.1006 - msle: 0.0096 - val_loss: 9.1927 - val_mape: 9.1927 - val_msle: 0.0133\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.1772 - mape: 5.1772 - msle: 0.0100 - val_loss: 8.9283 - val_mape: 8.9283 - val_msle: 0.0132\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.0507 - mape: 5.0507 - msle: 0.0095 - val_loss: 9.2792 - val_mape: 9.2792 - val_msle: 0.0134\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.2039 - mape: 5.2039 - msle: 0.0097 - val_loss: 9.3006 - val_mape: 9.3006 - val_msle: 0.0136\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.1657 - mape: 5.1657 - msle: 0.0098 - val_loss: 9.1423 - val_mape: 9.1423 - val_msle: 0.0131\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd95c2f4b20>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}