# Regularization

## Topics covered in today's module
* L1/L2 Regularization
* Dropout
* Overfitting
* Underfitting

## Main takeaways from doing today's assignment
* Regularization techniques such as L1/L2 regularization and Dropout can help prevent overfitting in neural networks.
* Overfitting occurs when a model learns to fit the training data too well and does not generalize well to new data, while underfitting occurs when a model is too simple and fails to capture the patterns in the data.
* Regularization can help to improve the performance of a model by constraining the weight values and reducing the complexity of the model.

## Challenging, interesting, or exciting aspects of today's assignment
* The challenging aspect was to understand the concepts of regularization and how they can be applied to neural networks. 
* The interesting aspect was seeing how regularization can help to improve the performance of a model and prevent overfitting.
## Additional resources used 
üìù[Dropout Regularization in Deep Learning Models With Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)<br>
