# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
* I got to know the different types of activation functions such as Sigmoid, Tanh, ReLU, and their advantages and disadvantages.
* I also learned about the vanishing and exploding gradients problem and the dying ReLU problem. 
* Lastly, I explored advanced activation functions such as Swish, GeLU, and SeLU.

## Challenging, interesting, or exciting aspects of today's assignment
* One challenging aspect was understanding the math behind the activation functions and the vanishing and exploding gradients problem. However, the visualizations provided in the assignment helped me understand more.
* The interesting aspect was learning about the dying ReLU problem and how it could be solved using Leaky ReLU.
## Additional resources used 
üìπ ["Activation Functions in Neural Networks" by StatQuest with Josh Starmer on YouTube](https://www.youtube.com/watch?v=-7scQpJT7uo)<br>
üìù ["Activation Functions in Neural Networks"](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)<br>

