# Optimization

## Topics covered in today's module
* Optimization
* Gradident Descent
* Optimizers(SGD, ADAM, etc.)

## Main takeaways from doing today's assignment
* Optimization is a critical part of training deep learning models.
* Gradient descent is a fundamental optimization algorithm in deep learning, but it has some limitations, such as getting stuck in local minima.
* Various optimization algorithms, such as SGD, Momentum, RMSProp, and Adam, have been proposed to overcome these limitations and improve the convergence of the  optimization process.
* The choice of optimization algorithm, learning rate, and other hyperparameters can have a significant impact on the performance of the model.

## Challenging, interesting, or exciting aspects of today's assignment
* The most challenging aspect of this assignment was understanding the theory behind optimization algorithms and how they work.
* The most exciting aspect was observing the differences in the convergence behavior of different optimization algorithms through the plotted loss/epoch curves.

## Additional resources used 
üìô ["Optimization for Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville](https://www.deeplearningbook.org/contents/optimization.html) chapter 8  which covers optimization algorithms for deep learning. The chapter explains various algorithms, including SGD, momentum, Adagrad, Adadelta, RMSprop, and Adam.<br>
üìù ["An Overview of Gradient Descent Optimization Algorithms" by Sebastian Ruder](https://ruder.io/optimizing-gradient-descent/)<br>
üìù ["CS231n: Convolutional Neural Networks for Visual Recognition" by Stanford University](https://cs231n.github.io/optimization-1/)
