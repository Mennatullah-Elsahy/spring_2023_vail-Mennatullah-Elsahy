# Overfitting and Underfitting

## Topics covered in today's module
* Overfitting
* Underfitting
* Data preparation
* Reducing network capacity

## Main takeaways from doing today's assignment
* Overfitting and underfitting are common problems in machine learning models, and they can lead to poor performance on new data.
* Regularization methods, such as L1/L2 regularization, dropout, and early stopping, can help prevent overfitting and underfitting.
* It's important to balance model complexity and generalization performance by adjusting the network's capacity, adding regularization methods, and tuning hyperparameters.
* Monitoring the training and validation loss curves can help diagnose and prevent overfitting and underfitting.

## Challenging, interesting, or exciting aspects of today's assignment
* The most challenging aspect of this assignment was understanding the concept of overfitting and underfitting and how to prevent them. 
* The most interesting and exciting part was implementing different regularization methods and comparing their performance to improve the model's generalization performance.
## Additional resources used 
üìì(Kaggle notebook)[https://www.kaggle.com/ryanholbrook/overfitting-and-underfitting]<br>
üìπ(DeepLearningAI)[https://www.youtube.com/watch?v=SjQyLhQIXSM]<br>
üìù(Article by EliteDataScience)[https://elitedatascience.com/overfitting-in-machine-learning]<br>
